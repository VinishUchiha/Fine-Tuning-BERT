{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "semantic_similarity_search_using_fine_tuned_bert.ipynb",
      "provenance": [],
      "mount_file_id": "1ahobM_M4pxAyC8i8EvtytcQOhuvS-R1K",
      "authorship_tag": "ABX9TyOjehzWSsQnDYhk7Vgf7LxO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VinishUchiha/Fine-Tuning-BERT/blob/master/Semantic_Similarity_Search/semantic_similarity_search_using_fine_tuned_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9_rORmpNixX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy the saved document classification model\n",
        "!cp -r /content/drive/My\\ Drive/BERT_doc_classification/saved_model /content/saved_model/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjEK58zWQMGR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "outputId": "52ad3a21-e83e-47fd-d151-c3b2ac873327"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 24.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a656a1f4093542c6becc3b972e2e62a25933ae0a8188ecb922d3e60d88fe6003\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfxTv124QYWN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8bd86b8b-99b5-4819-f27d-af25d56f6d8f"
      },
      "source": [
        "# check the gpu availability and set the device\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "  print('GPU :',torch.cuda.get_device_name(0))\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU : Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjfD18QzVzqS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "# download annotated comments and annotations\n",
        "\n",
        "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
        "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
        "\n",
        "\n",
        "def download_file(url, fname):\n",
        "    urllib.request.urlretrieve(url, fname)\n",
        "\n",
        "                \n",
        "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
        "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cbYy9wGV3ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the dataset\n",
        "import pandas as pd\n",
        "\n",
        "comments = pd.read_csv('attack_annotated_comments.tsv',sep='\\t',index_col = 0)\n",
        "annotations = pd.read_csv('attack_annotations.tsv',sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi2RdtQeWAvh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels a comment as an atack if the majority of annotators did so\n",
        "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5\n",
        "\n",
        "# join labels and comments\n",
        "comments['attack'] = labels\n",
        "\n",
        "# remove newline and tab tokens\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
        "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV2kA9NpOmck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "model_dir = '/content/saved_model'\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_dir,\n",
        "                                                      output_hidden_states=True)\n",
        "#load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WodCMaC9QKz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# text to embedding function\n",
        "def text_to_embedding(tokenizer,model,text):\n",
        "\n",
        "  MAX_LEN = 128\n",
        "  input_ids = tokenizer.encode(text,\n",
        "                               add_special_tokens=True,\n",
        "                               max_length = MAX_LEN)\n",
        "  results = pad_sequences([input_ids],maxlen=MAX_LEN,dtype='long',\n",
        "                          truncating='post',padding='post')\n",
        "  \n",
        "  input_ids = results[0]\n",
        "\n",
        "  attn_mask = [int(i>0) for i in input_ids]\n",
        "\n",
        "  # convert to tensors\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attn_mask = torch.tensor(attn_mask)\n",
        "\n",
        "  # add one extra dim\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  attn_mask = attn_mask.unsqueeze(0)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # move to GPU\n",
        "  input_ids = input_ids.to(device)\n",
        "  attn_mask = attn_mask.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    logits, encoded_layers = model(input_ids,token_type_ids=None,\n",
        "                                   attention_mask = attn_mask)\n",
        "    \n",
        "  layer = 12 # last bert layer before the classifier\n",
        "  batch = 0\n",
        "  token = 0\n",
        "\n",
        "  vec = encoded_layers[layer][batch][token]\n",
        "\n",
        "  # move to cpu\n",
        "  vec = vec.detach().cpu().numpy()\n",
        "\n",
        "  return (vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJP-_xyfU6Is",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6eb51118-a72d-43ac-a375-75a3d178d03e"
      },
      "source": [
        "# text from one of the comments\n",
        "input_text = comments.iloc[10].comment\n",
        "\n",
        "print(input_text)\n",
        "\n",
        "vec = text_to_embedding(tokenizer, model, input_text)\n",
        "\n",
        "print('Embedding Shape',vec.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  :Correct. Full biographical details will put down his birth details, etc. It is just a marker to me at the moment to detail the WR aspect. He certainly wasn't Belarus; as a geo-political entity it had no real existence at the time. I have put a tbc marker on this article for now. \n",
            "Embedding Shape (768,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aMPwIfpVpQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Helper function\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgdJwRUfXFf0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c82498f-96c3-465d-9a5f-80f3619eca67"
      },
      "source": [
        "t0 = time.time()\n",
        "\n",
        "embeddings = []\n",
        "\n",
        "num_comments = len(comments)\n",
        "\n",
        "print(f'Generating Sentence Embedding of {num_comments} comments')\n",
        "\n",
        "row_num = 0\n",
        "\n",
        "for index, row in comments.iterrows():\n",
        "  if row_num % 2000 == 0 and not row_num == 0:\n",
        "    elapsed = format_time(time.time() - t0)\n",
        "\n",
        "    # calculate the remaining time\n",
        "    row_per_sec = (time.time() - t0) / row_num\n",
        "    remaining_sec = row_per_sec * (num_comments - row_num)\n",
        "    remaining = format_time(remaining_sec)\n",
        "\n",
        "    print(f'  Comment {row_num} of {num_comments} Elapsed: {elapsed} Remaining: {remaining}')\n",
        "\n",
        "  # Vectorize the comment\n",
        "  vec = text_to_embedding(tokenizer,model,row.comment)\n",
        "\n",
        "  #store the embedding\n",
        "  embeddings.append(vec)\n",
        "\n",
        "  row_num += 1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating Sentence Embedding of 115864 comments\n",
            "  Comment 2000 of 115864 Elapsed: 0:00:23 Remaining: 0:21:32\n",
            "  Comment 4000 of 115864 Elapsed: 0:00:46 Remaining: 0:21:16\n",
            "  Comment 6000 of 115864 Elapsed: 0:01:08 Remaining: 0:20:50\n",
            "  Comment 8000 of 115864 Elapsed: 0:01:31 Remaining: 0:20:26\n",
            "  Comment 10000 of 115864 Elapsed: 0:01:53 Remaining: 0:19:53\n",
            "  Comment 12000 of 115864 Elapsed: 0:02:15 Remaining: 0:19:27\n",
            "  Comment 14000 of 115864 Elapsed: 0:02:38 Remaining: 0:19:07\n",
            "  Comment 16000 of 115864 Elapsed: 0:03:00 Remaining: 0:18:42\n",
            "  Comment 18000 of 115864 Elapsed: 0:03:22 Remaining: 0:18:21\n",
            "  Comment 20000 of 115864 Elapsed: 0:03:45 Remaining: 0:17:57\n",
            "  Comment 22000 of 115864 Elapsed: 0:04:07 Remaining: 0:17:33\n",
            "  Comment 24000 of 115864 Elapsed: 0:04:29 Remaining: 0:17:09\n",
            "  Comment 26000 of 115864 Elapsed: 0:04:50 Remaining: 0:16:43\n",
            "  Comment 28000 of 115864 Elapsed: 0:05:12 Remaining: 0:16:19\n",
            "  Comment 30000 of 115864 Elapsed: 0:05:34 Remaining: 0:15:56\n",
            "  Comment 32000 of 115864 Elapsed: 0:05:56 Remaining: 0:15:32\n",
            "  Comment 34000 of 115864 Elapsed: 0:06:18 Remaining: 0:15:09\n",
            "  Comment 36000 of 115864 Elapsed: 0:06:40 Remaining: 0:14:47\n",
            "  Comment 38000 of 115864 Elapsed: 0:07:02 Remaining: 0:14:25\n",
            "  Comment 40000 of 115864 Elapsed: 0:07:24 Remaining: 0:14:03\n",
            "  Comment 42000 of 115864 Elapsed: 0:07:46 Remaining: 0:13:40\n",
            "  Comment 44000 of 115864 Elapsed: 0:08:08 Remaining: 0:13:17\n",
            "  Comment 46000 of 115864 Elapsed: 0:08:30 Remaining: 0:12:55\n",
            "  Comment 48000 of 115864 Elapsed: 0:08:52 Remaining: 0:12:33\n",
            "  Comment 50000 of 115864 Elapsed: 0:09:15 Remaining: 0:12:11\n",
            "  Comment 52000 of 115864 Elapsed: 0:09:37 Remaining: 0:11:49\n",
            "  Comment 54000 of 115864 Elapsed: 0:10:00 Remaining: 0:11:28\n",
            "  Comment 56000 of 115864 Elapsed: 0:10:22 Remaining: 0:11:05\n",
            "  Comment 58000 of 115864 Elapsed: 0:10:44 Remaining: 0:10:43\n",
            "  Comment 60000 of 115864 Elapsed: 0:11:06 Remaining: 0:10:20\n",
            "  Comment 62000 of 115864 Elapsed: 0:11:28 Remaining: 0:09:57\n",
            "  Comment 64000 of 115864 Elapsed: 0:11:49 Remaining: 0:09:35\n",
            "  Comment 66000 of 115864 Elapsed: 0:12:11 Remaining: 0:09:13\n",
            "  Comment 68000 of 115864 Elapsed: 0:12:33 Remaining: 0:08:50\n",
            "  Comment 70000 of 115864 Elapsed: 0:12:56 Remaining: 0:08:28\n",
            "  Comment 72000 of 115864 Elapsed: 0:13:18 Remaining: 0:08:06\n",
            "  Comment 74000 of 115864 Elapsed: 0:13:40 Remaining: 0:07:44\n",
            "  Comment 76000 of 115864 Elapsed: 0:14:02 Remaining: 0:07:22\n",
            "  Comment 78000 of 115864 Elapsed: 0:14:24 Remaining: 0:06:59\n",
            "  Comment 80000 of 115864 Elapsed: 0:14:46 Remaining: 0:06:37\n",
            "  Comment 82000 of 115864 Elapsed: 0:15:08 Remaining: 0:06:15\n",
            "  Comment 84000 of 115864 Elapsed: 0:15:31 Remaining: 0:05:53\n",
            "  Comment 86000 of 115864 Elapsed: 0:15:53 Remaining: 0:05:31\n",
            "  Comment 88000 of 115864 Elapsed: 0:16:15 Remaining: 0:05:09\n",
            "  Comment 90000 of 115864 Elapsed: 0:16:37 Remaining: 0:04:47\n",
            "  Comment 92000 of 115864 Elapsed: 0:16:59 Remaining: 0:04:24\n",
            "  Comment 94000 of 115864 Elapsed: 0:17:22 Remaining: 0:04:02\n",
            "  Comment 96000 of 115864 Elapsed: 0:17:44 Remaining: 0:03:40\n",
            "  Comment 98000 of 115864 Elapsed: 0:18:06 Remaining: 0:03:18\n",
            "  Comment 100000 of 115864 Elapsed: 0:18:28 Remaining: 0:02:56\n",
            "  Comment 102000 of 115864 Elapsed: 0:18:50 Remaining: 0:02:34\n",
            "  Comment 104000 of 115864 Elapsed: 0:19:12 Remaining: 0:02:11\n",
            "  Comment 106000 of 115864 Elapsed: 0:19:35 Remaining: 0:01:49\n",
            "  Comment 108000 of 115864 Elapsed: 0:19:57 Remaining: 0:01:27\n",
            "  Comment 110000 of 115864 Elapsed: 0:20:19 Remaining: 0:01:05\n",
            "  Comment 112000 of 115864 Elapsed: 0:20:41 Remaining: 0:00:43\n",
            "  Comment 114000 of 115864 Elapsed: 0:21:03 Remaining: 0:00:21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzAdL8d1c8Nl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c3f3fcd-71ee-4f2e-82f6-51e03ff25d45"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# convert the list of vec into 2D array\n",
        "vecs = np.stack(embeddings)\n",
        "\n",
        "vecs.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115864, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v4mM8nbeaAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f62dda3e-6e1d-490b-d081-3e5684dd0fcb"
      },
      "source": [
        "# k-NN with faiss(Facebook AI Similarity Search)\n",
        "!pip install faiss"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faiss\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/1c/4ae6cb87cf0c09c25561ea48db11e25713b25c580909902a92c090b377c0/faiss-1.5.3-cp36-cp36m-manylinux1_x86_64.whl (4.7MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss) (1.18.5)\n",
            "Installing collected packages: faiss\n",
            "Successfully installed faiss-1.5.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqFo8v1pe0HS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "9a0e466b-9072-4e3a-b00d-20a2ddb500f0"
      },
      "source": [
        "!pip install faiss-gpu"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting faiss-gpu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/69/0e3f56024bb1423a518287673071ae512f9965d1faa6150deef5cc9e7996/faiss_gpu-1.6.3-cp36-cp36m-manylinux2010_x86_64.whl (35.5MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5MB 89kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from faiss-gpu) (1.18.5)\n",
            "Installing collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E21INJDXe27x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ba6c6922-7c7e-4227-9b1b-f6402c00408a"
      },
      "source": [
        "import faiss\n",
        "\n",
        "# build a flat cpu index\n",
        "cpu_index = faiss.IndexFlatL2(vecs.shape[1])\n",
        "\n",
        "print('Number of Available GPUs : ',faiss.get_num_gpus())\n",
        "\n",
        "# for multiple GPU\n",
        "co = faiss.GpuMultipleClonerOptions()\n",
        "co.shard = True\n",
        "\n",
        "# Make it into gpu index\n",
        "gpu_index = faiss.index_cpu_to_all_gpus(cpu_index,co = co,ngpu = 1)\n",
        "\n",
        "# add vec to our gpu index\n",
        "t0 = time.time()\n",
        "gpu_index.add(vecs)\n",
        "elapsed = time.time() - t0\n",
        "print(f'Time Taken to add vec: {elapsed}')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Available GPUs :  1\n",
            "Time Taken to add vec: 0.07732367515563965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD0PUylle23Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "bf590e42-d312-432a-f4e5-aeffc3513ea4"
      },
      "source": [
        "# Semantic Similarity Search\n",
        "print(f'Comment #4: {comments.iloc[4].comment}')\n",
        "\n",
        "#find top 5 similar content\n",
        "D,I = gpu_index.search(vecs[4].reshape(1,768),k=5)\n",
        "\n",
        "print(\"   Top 5 Results   \")\n",
        "\n",
        "for i in range(I.shape[1]):\n",
        "  result = I[0,i]\n",
        "  text = comments.iloc[result].comment\n",
        "  print(f'Comment Number {result}')\n",
        "  print(f'L2 Distance {D[0,i]}')\n",
        "  print(text)\n",
        "  print()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comment #4: This page will need disambiguation. \n",
            "   Top 5 Results   \n",
            "Comment Number 4\n",
            "L2 Distance 0.0\n",
            "This page will need disambiguation. \n",
            "\n",
            "Comment Number 2872\n",
            "L2 Distance 13.7762451171875\n",
            "DISAMBIGUATION PAGE needed  \n",
            "\n",
            "Comment Number 39578\n",
            "L2 Distance 14.284393310546875\n",
            "  This page needs to be expand.   \n",
            "\n",
            "Comment Number 45760\n",
            "L2 Distance 14.841827392578125\n",
            "So what is m? This page fails to define it.\n",
            "\n",
            "Comment Number 77417\n",
            "L2 Distance 15.710906982421875\n",
            "       A couple of these images should be added to the article.   \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CEVMlnie2wm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "ebd5c370-5159-4d48-d180-32a2ca679da5"
      },
      "source": [
        "# query a text\n",
        "query_text = 'the content in this page is fake'\n",
        "\n",
        "# vectorize the text\n",
        "query_vec = text_to_embedding(tokenizer,model,query_text)\n",
        "\n",
        "#find top 5 similar content\n",
        "D,I = gpu_index.search(query_vec.reshape(1,768),k=5)\n",
        "\n",
        "print(\"   Top 5 Results   \")\n",
        "\n",
        "for i in range(I.shape[1]):\n",
        "  result = I[0,i]\n",
        "  text = comments.iloc[result].comment\n",
        "  print(f'Comment Number {result}')\n",
        "  print(f'L2 Distance {D[0,i]}')\n",
        "  print(text)\n",
        "  print()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Top 5 Results   \n",
            "Comment Number 97265\n",
            "L2 Distance 43.85711669921875\n",
            "  This pages is not accurate it is full of information that refer to jokes and not actual data. Abstain from using it for any project. I recomend for this page to be locked    JP\n",
            "\n",
            "Comment Number 94394\n",
            "L2 Distance 45.39288330078125\n",
            "I don't think this page has any issues !\n",
            "\n",
            "Comment Number 90833\n",
            "L2 Distance 45.830902099609375\n",
            " :Where is that permission given? The linked page does not contain any such statement. There is no evidence of any CC-BY-SA licence on that page either.   \n",
            "\n",
            "Comment Number 35394\n",
            "L2 Distance 45.919952392578125\n",
            "  == Delete == I think this page should be deleted its obviously not in standard for wikipedia.\n",
            "\n",
            "Comment Number 53121\n",
            "L2 Distance 46.393035888671875\n",
            "  : This article is completely fake. Pyrrhus was ILLIRYAN, NOT GREEK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OBKw9-7lZNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}